{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glob/intel-python/versions/2018u2/intelpython3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#File name: SmallDenseNetwork \n",
    "#Names of all members of the group: James Kaufman, Eamon Kostopulos, \n",
    "#Ahmed Mohammed, Sebastian Cortes, and Jonathan Goral \n",
    "#Project name and description: DL Music Classification - A neural network that classifies music genres\n",
    "#Any special execution instruction: librosa, numpy, pydub, os, pickle, pylab, ffmeg, glob, matplotlib.pyplot\n",
    "#Date: 12/4/18\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras import initializers\n",
    "\n",
    "import librosa.feature\n",
    "import librosa.display\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code taken from https://blog.manash.me/building-a-dead-simple-word-recognition-engine-using-convnet-in-keras-25e72c19c12b\n",
    "def encodeGenres(genrelist):\n",
    "    uniq_ids, row_ids = np.unique(genrelist, return_inverse = True)\n",
    "    row_ids = row_ids.astype(np.int32, copy = False)\n",
    "    hot_labels = to_categorical(row_ids, len(uniq_ids))\n",
    "    #print(\"hot_labels\")\n",
    "    #print(hot_labels)\n",
    "    return hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop\n",
      "Instrumental\n",
      "Hip-Hop\n",
      "Rock\n",
      "International\n",
      "Folk\n",
      "Electronic\n",
      "[0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0.]\n",
      "Shape of x_test: (8395, 128, 217, 1)\n",
      "Shape of y_test: (8395, 7)\n",
      "Shape of x_train: (33578, 128, 217, 1)\n",
      "Shape of y_train: (33578, 7)\n"
     ]
    }
   ],
   "source": [
    "#SpecPickle is the name of the folder that we stored all of our pickle files in\n",
    "\n",
    "genre_files = os.listdir(\"SpecPickle\")\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "#load the data while evenly splitting each genre into test and train\n",
    "\n",
    "train_ratio = .8\n",
    "num_genre = len(genre_files)\n",
    "for file in genre_files:\n",
    "    #take off the \".pkl\" from the end of the file name\n",
    "    genre = file[:-4]\n",
    "    with open(\"SpecPickle/\" + file, 'rb') as f:\n",
    "        temp = pickle.load(f)\n",
    "        train_count = int(len(temp)*train_ratio)\n",
    "        count = 0\n",
    "        #split the data that we are adding in according to the training ratio we picked\n",
    "        for data in temp:\n",
    "            if count < train_count:\n",
    "                x_train.append(data)\n",
    "                y_train.append(genre)\n",
    "            else:\n",
    "                x_test.append(data)\n",
    "                y_test.append(genre)\n",
    "            count+=1\n",
    "            \n",
    "#convert the lists to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "x_test = np.asarray(x_test)\n",
    "\n",
    "\n",
    "\n",
    "#reshape to allow it to be used on the 2D CNN\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "\n",
    "#This all takes care of making a mapping that shows the labels that the network has\n",
    "#Given each genre.  We need this later to be able to test the network.\n",
    "\n",
    "listGenres = {}\n",
    "#these numbers are related to the fact that we have 5 genres with roughly 5000 snippets\n",
    "#to train off of in each genre\n",
    "for i in range(5):\n",
    "    listGenres[i] = y_train[i*5000]\n",
    "\n",
    "y_train = encodeGenres(y_train)\n",
    "y_test = encodeGenres(y_test)\n",
    "\n",
    "#actually create the mapping by looking at the new label for a particular genre\n",
    "#and then storing it in a dictionary with the genre name as a key.\n",
    "genreMapping = {}\n",
    "for i in range(5):\n",
    "    temp = y_train[i*5000]\n",
    "    position = 0\n",
    "    for j in range(len(temp)):\n",
    "        if temp[j] == 1:\n",
    "            position = j\n",
    "            j = len(temp)+1\n",
    "    genreMapping[listGenres[i]] = position\n",
    "\n",
    "#save this mapping\n",
    "with open (\"mapping.pkl\", 'wb') as f:\n",
    "    pickle.dump(genreMapping, f)\n",
    "\n",
    "print(\"Shape of x_test: \" + str(x_test.shape))\n",
    "print(\"Shape of y_test: \" + str(y_test.shape))\n",
    "print(\"Shape of x_train: \" + str(x_train.shape))\n",
    "print(\"Shape of y_train: \" + str(y_train.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our convolutional neural network, which has multiple convolutional layers\n",
    "#with max pooling between them as well as two dense layers at the end.\n",
    "#Activation functions and weight initialization were inspired by other projects that had worked well\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size = (2,2), activation = 'elu', kernel_initializer='glorot_uniform', input_shape=(x_train.shape[1],x_train.shape[2],x_train.shape[3])))\n",
    "model.add(MaxPooling2D(pool_size=(2,3)))\n",
    "model.add(Conv2D(128, kernel_size = (2,2), activation = 'elu', kernel_initializer='glorot_uniform'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(256, kernel_size = (2,2), activation = 'elu', kernel_initializer='glorot_uniform'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Conv2D(512, kernel_size = (2,2), activation = 'elu', kernel_initializer='glorot_uniform'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'elu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(num_genre, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 127, 216, 64)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 63, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 62, 71, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 31, 35, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 34, 256)       131328    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 15, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 16, 512)       524800    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 28672)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               7340288   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 2056      \n",
      "=================================================================\n",
      "Total params: 8,031,688\n",
      "Trainable params: 8,031,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'RMSprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38373 samples, validate on 9594 samples\n",
      "Epoch 1/30\n",
      " 3584/38373 [=>............................] - ETA: 30:03 - loss: 13.7554 - acc: 0.1342"
     ]
    }
   ],
   "source": [
    "#due to the large amount of data the network began to overfit after 6-8 epochs.\n",
    "model.fit(x_train, y_train, epochs = 8, batch_size =128, validation_data=(x_test, y_test), verbose = 2)\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"5genreweights.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
